\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}	
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}

\pagestyle{fancy}
\rhead{Home Assignment 2}
\lhead{Noah Hansson \& Kristoffer Nordström}

\title{Home Assignment 2 - FMSN50}
\author{Kristoffer Nordström, Noah Hansson}\author{Kristoffer Nordström \\ kr8245no-s@student.lu.se \and  Noah Hansson \\ no3822ha-s@student.lu.se}
\date{\today}


\setlength{\parskip}{0.7em}
\setlength{\parindent}{0pt}
\setlength{\floatsep}{6pt plus 1.0pt minus 2.0pt}
\setlength{\textfloatsep}{10pt plus 1.0pt minus 2.0pt}

\begin{document}
\maketitle
\newpage

\section*{1)}

All possible Self Avoiding Walks (SAW), $S_{n+m}(d)$, could be constructed by combining n-step SAWs with m-step SAWs, but all of these combination are not certain to
 be self avoiding and thus is

\begin{equation}
    \label{eq:multiplicity}
    c_{n+m}(d)\leq c_n(d)c_m(d)
\end{equation}

\section*{2)}
From the result in section 1 we can prove that the logarithm of $c_n$ is subadditive:
\begin{equation}
    \begin{gathered}
        c_{n+m}(d) \leq c_n(d)c_m(d) \\
        \iff \\
        log(c_{n+m}) \leq log(c_n(d)c_m(d)) = log(c_n(d)) + log(c_m(d))
    \end{gathered}
\end{equation}

Therefore, we can use \textit{Fekete's Lemma} to prove that $\lim_{n\rightarrow\infty} \frac{log(c_n(d))}{n}$ exists. Since $e^x$ is a well defined function for all $x$, we know that the limit $\lim_{n\rightarrow\infty} exp(\frac{log(c_n(d))}{n}) = \lim_{n\rightarrow\infty} c_n(d)^{1/n}$ exists. 

\section*{3)}
A first approach for estimating $c_n(2)$ is to generate $N$ random walks in $\mathbb{Z}^2$, and then count the amount of walks $N_{SA}$ that randomly happen to be self-avoiding. To sample random walks $X_i^{0:n}$ we draw 
\begin{equation}
    X_i^{n+1} \sim g_{n+1}(x_{n+1}|X^{0:n})
\end{equation}
where $g_{n+1}$ is the probability density of choosing a neighbouring point to the last point $X_i^n$. Since the walk is purely random, $g_{n+1}(x_{n+1}|X^{0:n}) = g_{n+1}(x_{n+1}|X^n)$ simply amounts to uniformly choosing a neigbour of $X^n$ with probability $p = \frac{1}{2d}$.

For every step each walk gets assigned a weight based on the probability of the walk resulting in this particular walk. The weights are defined such that:
\begin{equation}
    \sum_{i = 1}^N\frac{\omega_n^i}{\sum_{l=1}^N\omega_n^l}\phi(X_i^{0:n}) \approx \mathbb{E}_{f_n}(\phi(X_{0:n})) 
\end{equation}

All walks are at $n = 0$ assigned equal weights: $\omega_0^i = 1$, $\forall i$, and are then for each step updated as:
\begin{equation}
    \label{eq:weights}
    \omega_{n+1}^i = \frac{z_{n+1}(X_i^{0:n+1})}{z_{n}(X_i^{0:n})g_{n+1}(x_{n+1}|X^{0:n})} * \omega_n^i
\end{equation}
where $z_n(X_i^{0:n})$ is an indicator function with value $1$ if the walk if self-avoiding or $0$ otherwise.

When the weights are updated two things can happen: either the walk has collided with itself and the weight is set to $0$, or the walk continues to be self-avoiding and is divided by the probability density. Therefore, the weights are either $\omega_n^i = (2d)^n$ or zero. Because of this it's possible to write:

\begin{equation}
   \frac{1}{N}\sum_{i=1}^N\omega_n^i = (2d)^n \frac{N_{SA}}{N} = \tilde{c}_n(d)
\end{equation}

Using $N = 10^4$ approximations of $c_n(2)$ are presented in table \ref{tab:random_walks_results}.

\begin{table}[H]
    \centering
    \caption{Approximations of $c_n(2)$ for different n, with a $95\%$ confidence interval by sampling random walks.}
    \label{tab:random_walks_results}
    \include{tables/random_walk_results}
\end{table}

In table \ref{tab:random_walks_results} we can see that as $n$ increases, our estimates get worse. For large n, we can no longer estimate $c_n(2)$ as the probability of producing a self-avoiding walk randomly approaches zero. Instead, we need to find a method of increasing the probability of sampling a self-avoiding random walk.

\section*{4)}

To ensure that more sampled walks are self-avoiding, we can can update the distribution $g_{n+1}(x_{n+1}|X^{0:n})$ to only select neighbouring points that the walk has not visited before. By avoiding colliding walks, we can focus the computational power on the walks that are self-avoiding, increasing the ratio $N_{SA}/N$. 

For each particle, new weights are calculated with (\ref{eq:weights}). If $X_i^{0:n}$ don't have any possible neighbours, $X_i^{n+1}$ will result in a probability of zero and the corresponding weight for that particle will also be zero.

By using this approach, more of the sampled walks will be self-avoiding, making the ratio $N_{SA}/N$ larger. However, the weights will on average be smaller than the weights in task 3 since the probability density of $g_{n+1}(x_{n+1}|X^{0:n})$ is larger. Therefore $c_n(d)$ can still be approximated as:

\begin{equation}
    \tilde{c}_n(d) = \frac{1}{N}\sum_{i=1}^N\omega_n^i
\end{equation}

Using $N = 10^4$ approximations of $c_n(2)$ are presented in table \ref{tab:self_avoiding_results}.

\begin{table}[H]
    \centering
    \caption{Approximations of $c_n(2)$ for different n, with a $95\%$ confidence interval by sampling random walks.}
    \label{tab:self_avoiding_results}
    \include{tables/self_avoding_results}
\end{table}

\section*{5)}
To improve on the results in task 4 we can introduce resampling to the importance sampling. For each step $k$, we replace all particles $X_i^{0:k}$ with randomly selected copies of the earlier particles $\widetilde{X}_i^{0:k}$, distributed by the weights of each particle. Particles with a larger weight have a higher probability of being copied. This further focuses the computing power to those particles that are not only self-avoiding, but also likely to happen. After the particles are resampled, all weights are set to equal.

When using resampling it can be proved\footnote{From page 23 of the slides for lecture 7} that $c_n(d)$ can be approximated by:
\begin{equation}
    c_n(d) = \prod_{k = 0}^n(\frac{1}{N}\sum_{i=1}^N\omega_k^i)
\end{equation}

For each particle, new weights is calculated with (\ref{eq:weights}). If $X^{n}$ don't have any possible neighbours, $X^{n+1}$ is selected as $X^n$, this will result in a probability of zero and that the corresponding weight for that particle will be zero, killing the particle.

This approach will give better estimates of $c_n(d)$ for large n compared to the previous section. However, the variance will be slightly higher due to the randomness introduced in the resampling.

Using $N = 10^4$ approximations of $c_n(2)$ are presented in table \ref{tab:resampling_results}.

\begin{table}[H]
    \centering
    \caption{Approximations of $c_n(2)$ for different n, with a $95\%$ confidence interval by sampling random walks.}
    \label{tab:resampling_results}
    \include{tables/resampling_results}
\end{table}

\section*{6)}

It is conjectured for all $d$ that:
\begin{equation}
    \label{eq:cd}
    c_n(d) \sim 
    \begin{cases}
        A_d\mu_d^nn^{\gamma-1}, \quad d = 1, 2, 3, d \geq 5 \\
        A_d\mu_d^n\log(n)^{1/4}, \quad d = 4
    \end{cases}, \quad as \; n \to \infty
\end{equation}

We are interested in estimating the parameters $A_d$, $\mu_d$ and $\gamma$ for $d = 2$.
By taking the logarithm (\ref{eq:cd}) when $d=2$ the following equation is derived:
\begin{equation}
    \ln(c_n(d)) =  \ln(A_d)+\ln(\mu_d)n+\ln(n)({\gamma-1})
    \label{eq:ln_cn}
\end{equation}

Rewriting (\ref{eq:ln_cn}) on matrix form yields:

\begin{equation}
    \begin{gathered}
        \ln(c_n)=
        \begin{pmatrix}
            \ln(A_2) & \ln(\mu_2) & (\gamma -1)
        \end{pmatrix}
        \begin{pmatrix}
            1 \\
            n \\
            \ln(n)
        \end{pmatrix}
        \\
        \iff
        \\
        y = \theta X
    \end{gathered}
\end{equation}

The estimation of the parameters, $\theta$, is done by simulating $c_n(2)$ for all $n$ up to 100. The optimal regression of the parameters is then calculated with the normal equation:

\begin{equation}
    \theta = (X^T X)^{-1}X^T y    
\end{equation}

The estimated parameters can be seen in table \ref{tab:estimate_parameters}. The theoretical value for $\gamma_2$ is $43/32 \approx 1.34$. As $n$ gets larger the estimate of $\gamma$ gets closer to the theoretical value.

\begin{table}[H]
    \centering
    \caption{Approximations of $\theta$ for different n. The step length is 100 and the number of samples are $10^4$}
    \label{tab:estimate_parameters}
    \include{tables/parameters_2d}
\end{table}

We are also interested in finding the variance of the estimate of $\theta$. This is done by simulating 10 independent estimates of $\theta$ and then taking the variance of the estimates. The results are presented in table \ref{tab:multiple_theta} and \ref{tab:multiple_theta_variance}. In table \ref{tab:multiple_theta_variance} we find that $\mu_2$ has the lowest variane, followed by $\gamma_2$, and last comes $A_2$. The reason for the differences in variance is the speed of convergence for each estimate as $n \to \infty$. Since $\mu_2$ is dependant on $n$ it converges faster than $\gamma_2$ which is dependant on $\ln(n)$. $A_2$ is not dependant on $n$ and will not converge as $n$ grows.

\begin{table}[H]
    \centering
    \caption{Approximations of $\theta$ for multiple runs. The step length is 100 and the number of samples are $10^4$}
    \label{tab:multiple_theta}
    \include{tables/multiple_theta}
\end{table}

\begin{table}[H]
    \centering
    \caption{Variance of approximations of $\theta$ for multiple runs. The step length is 100 and the number of samples are $10^4$}
    \label{tab:multiple_theta_variance}
    \include{tables/multiple_theta_variance}
\end{table}

\section*{7)}

To find a bound for $\mu_d$ we can use the following equation:

\begin{equation}
    \label{eq:limit}
     \mu_d = \lim_{n \to \infty} c_d(n)^{1/n}
\end{equation}


First, we begin by finding an upper and a lower bound for $c_d(n)$. For the upper bound, we know that the maximum amount of self-avoiding random walks is bounded by the maximum possible amount of random walks, $(2d)^n$. Furthermore, we can extend this reasoning to also exclude the walks when a step simply retraces its previous step, reducing the upper bound of self-avoiding random walks to $(2d-1)^n$. For the lower bound, we know that all random walks that only step in positive directions will be self-avoiding. Therefore we find a lower bound of $d^n$. This gives us the expression \begin{equation}
    d^n \leq c_d(n) \leq (2d-1)^n
\end{equation}
We then take the limit as in equation (\ref{eq:limit}) of the bounds as well as $c_n(d)$. This gives us:
\begin{equation}
    \lim_{n \to \infty} (d^n)^{1/n} \leq \lim_{n \to \infty} c_n(d)^{1/n} \leq \lim_{n \to \infty} ((2d-1)^n)^{1/n}
\end{equation}
Simplifying and inserting equation (\ref{eq:limit}) gives us:
\begin{equation}
    d \leq \mu_d \leq 2d-1
\end{equation}

\section*{8)}
We can find a lower bound for $A_d$ when $d \geq 5$ by using the equation:


For any integers $a$ and $b$ such that $a \to \infty$ and $b \to \infty$ we can plug in equation (\ref{eq:cd}) to equation (\ref{eq:multiplicity}) to get:
\begin{equation}
    A_d\mu_d^{a+b}(a+b)^{\gamma_d-1} = c_{a+b}(d)\leq c_a(d)c_b(d) = A_d\mu_d^aa^{\gamma-1}A_d\mu_d^bb^{\gamma-1}
\end{equation}
Since it is known that $\gamma = 1$ for $d \geq 5$ we can simplify this to:
\begin{equation}
    A_d\mu^{a+b} \leq A_d^2\mu^{a+b} \implies A_d \geq 1
\end{equation}
\section*{9)}

Using the parameter estimation approach from task 6) we can verify the results in 7) and 8) for some $d\geq 3$. We are also interested in verifying another result for estimating $\mu_d$:

\begin{equation}
    \label{eq:graham}
\mu_d \sim 2d-1-\frac{1}{2d} - \frac{3}{(2d)^2} - \frac{12}{(2d)^3} + O(\frac{1}{d^4})
\end{equation}

The results are presented in table \ref{tab:parameters_d}. Here we can see that the results in 7) and 8) hold, as well as the estimate of $\mu_d$. In fact, the estimate of $\mu$ is very close to the approximation in equation (\ref{eq:graham}).

\begin{table}[H]
    \centering
    \caption{Approximations of $\theta$ for different d. Simulations for each dimension was made with walk length of 100 and $10^4$ samples.}
    \label{tab:parameters_d}
    \include{tables/parameters_for_dimensions}
\end{table}

\end{document}
